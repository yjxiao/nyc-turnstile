{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Author: Yijun Xiao <ryjxiao@nyu.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Loader\n",
    "Created to help fetch data from MTA website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "from pandas import datetime, Timedelta\n",
    "import urllib2\n",
    "import os\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TurnstileDataLoader:\n",
    "    \"\"\"Automatically load data from MTA website\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # format of links to the txt files\n",
    "        self.url_base = \"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{0}.txt\"\n",
    "        # first day of data\n",
    "        self.begining_of_time = datetime(2010, 5, 1)\n",
    "        # date when format of data changed\n",
    "        self.new_era = datetime(2014, 10, 18)\n",
    "        self.today = datetime.today()\n",
    "        \n",
    "        # prepare station df for old format data\n",
    "        self.data_dir = \"data/\"\n",
    "        station_df_path = os.path.join(self.data_dir, \"station.pkl\")\n",
    "        if os.path.isfile(station_df_path):\n",
    "            with open(station_df_path) as f:\n",
    "                self.station_df = pickle.load(f)\n",
    "        else:\n",
    "            self.station_df = pd.read_excel(\"http://web.mta.info/developers/resources/nyct/turnstile/Remote-Booth-Station.xls\")\n",
    "            self.station_df.columns = [\"UNIT\", \"C/A\", \"STATION\", \"LINENAME\", \"DIVISION\"]\n",
    "            # save to data directory\n",
    "            if not os.path.exists(self.data_dir):\n",
    "                os.makedirs(self.data_dir)\n",
    "            with open(station_df_path, \"wb\") as f:\n",
    "                pickle.dump(self.station_df, f)\n",
    "        \n",
    "    def _next_saturday(self, date):\n",
    "        \"\"\"Find the nearest saturday after input date when data updated\"\"\"\n",
    "        weekday = date.weekday()\n",
    "        delta = Timedelta(7 - (weekday + 2) % 7, unit='d')\n",
    "        return date + delta\n",
    "    \n",
    "    def _find_files(self, start_date, end_date):\n",
    "        \"\"\"Find list of files covering specified starting date and \n",
    "        end date.\"\"\"\n",
    "        # some assertions\n",
    "        assert(start_date < end_date)\n",
    "        assert(start_date >= self.begining_of_time)\n",
    "        # assert data is available on mta website\n",
    "        assert(self._next_saturday(end_date) <= self.today)\n",
    "        \n",
    "        files = []\n",
    "        start_saturday = self._next_saturday(start_date)\n",
    "        end_saturday = self._next_saturday(end_date)\n",
    "        while start_saturday <= end_saturday:\n",
    "            datestr = start_saturday.strftime(\"%y%m%d\")\n",
    "            files.append(self.url_base.format(datestr))\n",
    "            start_saturday += Timedelta(\"7 days\")\n",
    "        return files\n",
    "    \n",
    "    def _load_old(self, txtfileurl):\n",
    "        \"\"\"Load old format txt file which needs quite some reformating\"\"\"\n",
    "        records = []\n",
    "        txtfile = urllib2.urlopen(txtfileurl)\n",
    "        for line in txtfile:\n",
    "            row = line.strip().split(\",\")\n",
    "            if len(row) < 8:\n",
    "                continue\n",
    "            ca, unit, scp = row[:3]\n",
    "            i = 3\n",
    "            while i < len(row):\n",
    "                date, time, desc, entries, exits = row[i:i+5]\n",
    "                date_time = datetime.strptime(date + \" \" + time, \"%m-%d-%y %H:%M:%S\")\n",
    "                record = dict(DATE_TIME=date_time, UNIT=unit, SCP=scp,\n",
    "                              DESC=desc, ENTRIES=int(entries), EXITS=int(exits))\n",
    "                record[\"C/A\"] = ca\n",
    "                records.append(record)\n",
    "                i += 5\n",
    "        old_df = pd.DataFrame.from_records(records)\n",
    "        return pd.merge(old_df, self.station_df, how=\"left\").set_index([\"DATE_TIME\"])\n",
    "        \n",
    "    def load(self, txtfileurl):\n",
    "        \"\"\"Load txt file specified by a url as a DataFrame\"\"\"\n",
    "        datestr = txtfileurl[-10:-4]\n",
    "        # check if we have the data saved already\n",
    "        filepath = os.path.join(self.data_dir, datestr + \".pkl\")\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath) as f:\n",
    "                result = pickle.load(f)\n",
    "        else:\n",
    "            # detect whether this file is of the newer, cleaner format\n",
    "            is_new = datetime.strptime(datestr, \"%y%m%d\") >= self.new_era\n",
    "            if is_new:\n",
    "                result = pd.read_csv(txtfileurl, parse_dates=[[6,7]], index_col=[\"DATE_TIME\"]).sort_index()\n",
    "                result.columns = [\"C/A\", \"UNIT\", \"SCP\", \"STATION\",\n",
    "                                  \"LINENAME\", \"DIVISION\", \"DESC\", \"ENTRIES\", \"EXITS\"]\n",
    "            else:\n",
    "                result = self._load_old(txtfileurl).sort_index()\n",
    "                \n",
    "            result[\"TURNSTILE_ID\"] = result[[\"C/A\", \"UNIT\", \"SCP\"]].apply(lambda x: \" \".join(x), axis=1)\n",
    "            result.drop([\"C/A\", \"UNIT\", \"SCP\"], axis=1, inplace=True)\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                pickle.dump(result, f)\n",
    "                \n",
    "        return result\n",
    "            \n",
    "    def retrieve(self, start_date, end_date):\n",
    "        \"\"\"Retrieve data given starting date and end date.\n",
    "        Note that starting date is included while end date is not.\n",
    "        \"\"\"\n",
    "        files = self._find_files(start_date, end_date)\n",
    "        frames = []\n",
    "        for fileurl in files:\n",
    "            frames.append(self.load(fileurl))\n",
    "        result = pd.concat(frames)\n",
    "        # need to include the first entry of end date to calculate count\n",
    "        # the latest first entry on any day is 3am\n",
    "        end_time = end_date.replace(hour=3, minute=0, second=0, microsecond=0)\n",
    "        return result[start_date:end_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Total number of entries & exits\n",
    "across the subway system for Aug. 1st, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_loader = TurnstileDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date = datetime(2013, 8, 1)\n",
    "end_date = datetime(2013, 8, 2)\n",
    "df = data_loader.retrieve(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read \"odometers\" at the beginning and the end\n",
    "start_count = df.ix[start_date][[\"TURNSTILE_ID\", \"STATION\", \"ENTRIES\", \"EXITS\"]]\n",
    "end_count = df.ix[end_date][[\"TURNSTILE_ID\", \"STATION\", \"ENTRIES\", \"EXITS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2360\n",
      "2365\n"
     ]
    }
   ],
   "source": [
    "# We have different number of turnstiles recorded at midnight\n",
    "# Aug. 1st and Aug. 2nd. Thus it's not okay to simply sum up\n",
    "# all entries and exits at both time points and do subtraction\n",
    "print(len(start_count))\n",
    "print(len(end_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the set of turnstiles in both time points\n",
    "turnstile_setA = set()\n",
    "turnstile_setB = set()\n",
    "for _, row in start_count.iterrows():\n",
    "    turnstile_setA.add(row[\"TURNSTILE_ID\"])\n",
    "for _, row in end_count.iterrows():\n",
    "    turnstile_setB.add(row[\"TURNSTILE_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate the intersection\n",
    "set_both = turnstile_setA.intersection(turnstile_setB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the mask for each df, only keep turnstiles in intersection\n",
    "maskA = start_count.apply(lambda x: x[\"TURNSTILE_ID\"] in set_both, axis=1)\n",
    "maskB = end_count.apply(lambda x: x[\"TURNSTILE_ID\"] in set_both, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can see here something is wrong with our data\n",
    "# as total odometer reading on Aug. 2nd is smaller\n",
    "odometerA = start_count[maskA][[\"ENTRIES\", \"EXITS\"]].sum()\n",
    "odometerB = end_count[maskB][[\"ENTRIES\", \"EXITS\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some odometers have been reset during the time frame\n",
    "bad_entries_diff = 0\n",
    "bad_exits_diff = 0\n",
    "bad_turnstiles = set()\n",
    "for tid in set_both:\n",
    "    rowA = start_count[start_count[\"TURNSTILE_ID\"] == tid]\n",
    "    rowB = end_count[end_count[\"TURNSTILE_ID\"] == tid]\n",
    "    if (rowA.ENTRIES.sum() > rowB.ENTRIES.sum()) | (rowA.EXITS.sum() > rowB.EXITS.sum()):\n",
    "        bad_entries_diff += rowA.ENTRIES.sum() - rowB.ENTRIES.sum()\n",
    "        bad_exits_diff += rowA.EXITS.sum() - rowB.EXITS.sum()\n",
    "        bad_turnstiles.add(tid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 5,704,970 entries and exits on 2013-08-01\n"
     ]
    }
   ],
   "source": [
    "# we ignore resetted odometers here. This count should be smaller than\n",
    "# real count\n",
    "total_traffic = (odometerB - odometerA).sum() + bad_entries_diff + bad_exits_diff\n",
    "print(\"There are a total of {0:,} entries and exits on {1}\"\\\n",
    "      .format(total_traffic, start_date.strftime(\"%Y-%m-%d\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Busiest station and turnstile\n",
    "on Aug. 1st, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# redefine masks to further filter out resetted turnstiles\n",
    "valid_turnstiles = set_both - bad_turnstiles\n",
    "maskA = start_count.apply(lambda x: x[\"TURNSTILE_ID\"] in valid_turnstiles, axis=1)\n",
    "maskB = end_count.apply(lambda x: x[\"TURNSTILE_ID\"] in valid_turnstiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sum of odometer readings grouped by station\n",
    "station_countA = start_count[maskA].groupby(\"STATION\").sum()\n",
    "station_countB = end_count[maskB].groupby(\"STATION\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total traffic of both entries and exits in each station\n",
    "station_traffic = (station_countB - station_countA).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "busiest_station = station_traffic.argmax()\n",
    "largest_station_traffic = station_traffic.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The busiest station on Aug. 1st, 2013 is 34 ST-HERALD SQ\n",
      "Total entries & exits: 237,077\n"
     ]
    }
   ],
   "source": [
    "print(\"The busiest station on Aug. 1st, 2013 is {0}\".format(busiest_station))\n",
    "print(\"Total entries & exits: {0:,}\".format(largest_station_traffic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turnstile_countA = start_count[maskA].groupby(\"TURNSTILE_ID\").sum()\n",
    "turnstile_countB = end_count[maskB].groupby(\"TURNSTILE_ID\").sum()\n",
    "turnstile_traffic = (turnstile_countB - turnstile_countA).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "busiest_turnstile = turnstile_traffic.argmax()\n",
    "largest_turnstile_traffic = turnstile_traffic.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check which station this turnstile is located\n",
    "at_station = start_count[start_count[\"TURNSTILE_ID\"]==busiest_turnstile][\"STATION\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The busiest turnstile on Aug. 1st, 2013 is N063A R011 00-00-00\n",
      "Located at 42 ST-PA BUS TE\n",
      "Total entries & exits: 11,845\n"
     ]
    }
   ],
   "source": [
    "print(\"The busiest turnstile on Aug. 1st, 2013 is {0}\".format(busiest_turnstile))\n",
    "print(\"Located at {0}\".format(at_station))\n",
    "print(\"Total entries & exits: {0:,}\".format(largest_turnstile_traffic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Busiest and least-busy stations\n",
    "in July, 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "start_date = datetime(2013, 7, 1)\n",
    "end_date = datetime(2013, 8, 1)\n",
    "df2 = data_loader.retrieve(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automate the masking process\n",
    "def get_valid_readings(start_df, end_df):\n",
    "    setA = set()\n",
    "    setB = set()\n",
    "    for _, row in start_df.iterrows():\n",
    "        setA.add(row[\"TURNSTILE_ID\"])\n",
    "    for _, row in end_df.iterrows():\n",
    "        setB.add(row[\"TURNSTILE_ID\"])\n",
    "    valid_set = setA.intersection(setB)\n",
    "    bad_set = set()\n",
    "    for tid in valid_set:\n",
    "        rowA = start_df[start_df[\"TURNSTILE_ID\"] == tid]\n",
    "        rowB = end_df[end_df[\"TURNSTILE_ID\"] == tid]\n",
    "        if (rowA.ENTRIES.sum() > rowB.ENTRIES.sum()) | (rowA.EXITS.sum() > rowB.EXITS.sum()):\n",
    "            bad_set.add(tid)\n",
    "    valid_set -= bad_set\n",
    "    maskA = start_df.apply(lambda x: x[\"TURNSTILE_ID\"] in valid_set, axis=1)\n",
    "    maskB = end_df.apply(lambda x: x[\"TURNSTILE_ID\"] in valid_set, axis=1)\n",
    "    return maskA, maskB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_df = df2.ix[start_date][[\"TURNSTILE_ID\", \"STATION\", \"ENTRIES\", \"EXITS\"]]\n",
    "end_df = df2.ix[end_date][[\"TURNSTILE_ID\", \"STATION\", \"ENTRIES\", \"EXITS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maskA, maskB = get_valid_readings(start_df, end_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same analysis as in part 2\n",
    "station_countA = start_df[maskA].groupby(\"STATION\").sum()\n",
    "station_countB = end_df[maskB].groupby(\"STATION\").sum()\n",
    "station_traffic = (station_countB - station_countA).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The busiest station in July, 2013 is 34 ST-HERALD SQ\n",
      "Total entries & exits: 6,198,212\n"
     ]
    }
   ],
   "source": [
    "busiest_station = station_traffic.argmax()\n",
    "largest_station_traffic = station_traffic.max()\n",
    "print(\"The busiest station in July, 2013 is {0}\".format(busiest_station))\n",
    "print(\"Total entries & exits: {0:,}\".format(largest_station_traffic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The least-busy station in July, 2013 is AQUEDUCT TRACK\n",
      "Total entries & exits: 269\n"
     ]
    }
   ],
   "source": [
    "least_station = station_traffic.argmin()\n",
    "smallest_station_traffic = station_traffic.min()\n",
    "print(\"The least-busy station in July, 2013 is {0}\".format(least_station))\n",
    "print(\"Total entries & exits: {0:,}\".format(smallest_station_traffic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Station with highest average number of entries \n",
    "between midnight & 4am on Fridays in July 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all data in the time frame of interest\n",
    "# Saturday 0am to 4am\n",
    "m = df2.index.map(lambda x: (x.weekday() == 5) and (x.time() <= time(4, 0, 0)))\n",
    "df3 = df2[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_times = pd.date_range('2013-07-06 00:00', periods=4, freq='W-SAT')\n",
    "end_times = pd.date_range('2013-07-06 04:00', periods=4, freq='W-SAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate entries for every Friday midnight - Saturday 4am\n",
    "station_entries = []\n",
    "for i in xrange(4):\n",
    "    start_df = df3.ix[start_times[i]]\n",
    "    end_df = df3.ix[end_times[i]]\n",
    "    start_df = start_df[start_df.DESC == \"REGULAR\"][[\"TURNSTILE_ID\", \"STATION\", \"ENTRIES\", \"EXITS\"]]\n",
    "    end_df = end_df[end_df.DESC == \"REGULAR\"][[\"TURNSTILE_ID\", \"STATION\", \"ENTRIES\", \"EXITS\"]]\n",
    "    mA, mB = get_valid_readings(start_df, end_df)\n",
    "    station_entriesA = start_df[mA].groupby(\"STATION\").ENTRIES.sum()\n",
    "    station_entriesB = end_df[mB].groupby(\"STATION\").ENTRIES.sum()\n",
    "    station_entries.append(station_entriesB - station_entriesA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "night_station = sum(station_entries).argmax()\n",
    "night_avg_entries = sum(station_entries).max() / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 ST-TIMES SQ has the highest average number of entries between midnight & 4am on Fridays in July 2013\n",
      "Averaging 7,818 entries per night.\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} has the highest average number of entries between midnight & 4am on Fridays in July 2013\".format(night_station))\n",
    "print(\"Averaging {0:,} entries per night.\".format(int(night_avg_entries)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
